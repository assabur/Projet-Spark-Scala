{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class Prenom\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case class Prenom(sexe: String, prenom: String, annee: Int, codeDept: Int, nombre: Int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "prenomsRDD: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[14] at filter at <console>:35\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val prenomsRDD = sc.textFile(\"prenoms.txt\").filter(l => l.startsWith(\"sexe\") == false).filter(l => l.contains(\"XX\") == false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sparkRO: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@6e3db2a1\n",
       "import sparkRO.implicits._\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Pour les conversions implicites de RDDs vers DataFrames\n",
    "val sparkRO = spark // bricolage pour que cela fonctionne dans le notebool (inutile sinon)\n",
    "import sparkRO.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "prenoms: org.apache.spark.sql.Dataset[Prenom] = [sexe: string, prenom: string ... 3 more fields]\n",
       "res3: prenoms.type = [sexe: string, prenom: string ... 3 more fields]\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val prenoms = prenomsRDD.map(_.split('\\t')).map(a => Prenom(a(0), a(1), a(2).toInt, a(3).toInt, a(4).toDouble.toInt)).toDS()\n",
    "prenoms.cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sauvegarde dans les différents formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import java.nio.file._\n",
       "import java.nio.file.attribute.BasicFileAttributes\n",
       "import java.util.concurrent.atomic.AtomicLong\n",
       "pathSize: (path: java.nio.file.Path)Long\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import java.nio.file._\n",
    "import java.nio.file.attribute.BasicFileAttributes\n",
    "import java.util.concurrent.atomic.AtomicLong\n",
    "\n",
    "def pathSize(path: Path): Long = {\n",
    "    var size = new AtomicLong(0)\n",
    "\n",
    "    Files.walkFileTree(path, new SimpleFileVisitor[Path]() {\n",
    "        override def visitFile(file: Path, attrs: BasicFileAttributes): FileVisitResult = {\n",
    "            size.addAndGet(attrs.size())\n",
    "            FileVisitResult.CONTINUE\n",
    "        }\n",
    "    });\n",
    "    size.get()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "formats: scala.collection.immutable.Map[String,List[String]] = Map(csv -> List(uncompressed, bzip2, deflate, gzip), json -> List(uncompressed, bzip2, deflate, gzip), parquet -> List(uncompressed, gzip, snappy), orc -> List(uncompressed, snappy, zlib))\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val formats = Map(\n",
    "    \"csv\" -> List(\"uncompressed\", \"bzip2\", \"deflate\", \"gzip\"),\n",
    "    \"json\" -> List(\"uncompressed\", \"bzip2\", \"deflate\", \"gzip\"),\n",
    "    \"parquet\" -> List(\"uncompressed\", \"gzip\", \"snappy\"),\n",
    "    \"orc\" -> List(\"uncompressed\", \"snappy\", \"zlib\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "org.apache.spark.SparkException",
     "evalue": " Job aborted.",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.SparkException: Job aborted.",
      "  at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:226)",
      "  at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:178)",
      "  at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)",
      "  at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)",
      "  at org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)",
      "  at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)",
      "  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)",
      "  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)",
      "  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)",
      "  at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:122)",
      "  at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:121)",
      "  at org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:963)",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)",
      "  at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:963)",
      "  at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:415)",
      "  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:399)",
      "  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:288)",
      "  at $anonfun$res4$2(<console>:54)",
      "  at $anonfun$res4$2$adapted(<console>:53)",
      "  at scala.collection.immutable.List.foreach(List.scala:392)",
      "  at $anonfun$res4$1(<console>:53)",
      "  at $anonfun$res4$1$adapted(<console>:52)",
      "  at scala.collection.Iterator.foreach(Iterator.scala:941)",
      "  at scala.collection.Iterator.foreach$(Iterator.scala:941)",
      "  at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)",
      "  at scala.collection.MapLike$DefaultKeySet.foreach(MapLike.scala:181)",
      "  ... 45 elided",
      "Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 1.0 failed 1 times, most recent failure: Lost task 1.0 in stage 1.0 (TID 4, d043b0c5c377, executor driver): java.lang.ArrayIndexOutOfBoundsException: Index 1 out of bounds for length 1",
      "\tat $anonfun$prenoms$2(<console>:45)",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$1.hasNext(InMemoryRelation.scala:132)",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1371)",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1298)",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1362)",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1186)",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:360)",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:311)",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)",
      "",
      "Driver stacktrace:",
      "  at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)",
      "  at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)",
      "  at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)",
      "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)",
      "  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)",
      "  at scala.Option.foreach(Option.scala:407)",
      "  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)",
      "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)",
      "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)",
      "  at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:195)",
      "  ... 75 more",
      "Caused by: java.lang.ArrayIndexOutOfBoundsException: Index 1 out of bounds for length 1",
      "  at $anonfun$prenoms$2(<console>:45)",
      "  at scala.collection.Iterator$$anon$10.next(Iterator.scala:459)",
      "  at scala.collection.Iterator$$anon$10.next(Iterator.scala:459)",
      "  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)",
      "  at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)",
      "  at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)",
      "  at org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$1.hasNext(InMemoryRelation.scala:132)",
      "  at org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)",
      "  at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)",
      "  at org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1371)",
      "  at org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1298)",
      "  at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1362)",
      "  at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1186)",
      "  at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:360)",
      "  at org.apache.spark.rdd.RDD.iterator(RDD.scala:311)",
      "  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)",
      "  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)",
      "  at org.apache.spark.rdd.RDD.iterator(RDD.scala:313)",
      "  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)",
      "  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)",
      "  at org.apache.spark.rdd.RDD.iterator(RDD.scala:313)",
      "  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)",
      "  at org.apache.spark.scheduler.Task.run(Task.scala:127)",
      "  at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)",
      "  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)",
      "  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)",
      "  at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)",
      "  at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)",
      "  ... 1 more",
      ""
     ]
    }
   ],
   "source": [
    "for (format <- formats.keys) {\n",
    "    for (codec <- formats(format)) {\n",
    "        prenoms.write.mode(\"overwrite\").option(\"compression\", codec).format(format).save(\"prenoms\")\n",
    "        val prenomsPath = Paths.get(\"prenoms\")\n",
    "        println(s\"$format, $codec, \" + pathSize(prenomsPath))\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Affichage des résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "resultats: String =\n",
       "csv, uncompressed, 68804952\n",
       "csv, bzip2, 10676889\n",
       "csv, deflate, 11998839\n",
       "csv, gzip, 11998875\n",
       "json, uncompressed, 238735998\n",
       "json, bzip2, 9491007\n",
       "json, deflate, 14662059\n",
       "json, gzip, 14662095\n",
       "parquet, uncompressed, 8344008\n",
       "parquet, gzip, 4820150\n",
       "parquet, snappy, 6476035\n",
       "orc, uncompressed, 13265926\n",
       "orc, snappy, 5810818\n",
       "orc, zlib, 4377612\n",
       "sizes: Array[String] = Array(\"csv, uncompressed, 68804952                                                     \", \"csv, bzip2, 10676889                                                            \", \"csv, deflate, 11998839          \", \"csv, gzip, 11998875                                                             \", \"json, uncompressed, 238735998                                                   \", \"json, bzip2, 9491007                    ...\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val resultats = \"\"\"csv, uncompressed, 68804952                                                     \n",
    "csv, bzip2, 10676889                                                            \n",
    "csv, deflate, 11998839          \n",
    "csv, gzip, 11998875                                                             \n",
    "json, uncompressed, 238735998                                                   \n",
    "json, bzip2, 9491007                                                            \n",
    "json, deflate, 14662059                                                         \n",
    "json, gzip, 14662095                                                            \n",
    "parquet, uncompressed, 8344008                                                  \n",
    "parquet, gzip, 4820150                                                          \n",
    "parquet, snappy, 6476035                                 \n",
    "orc, uncompressed, 13265926                                                     \n",
    "orc, snappy, 5810818                                                            \n",
    "orc, zlib, 4377612\"\"\"\n",
    "\n",
    "val sizes = resultats.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Map((orc,snappy) -> 5810818, (json,deflate) -> 14662059, (json,bzip2) -> 9491007, (csv,uncompressed) -> 68804952, (parquet,snappy) -> 6476035, (csv,deflate) -> 11998839, (csv,gzip) -> 11998875, (json,uncompressed) -> 238735998, (parquet,gzip) -> 4820150, (orc,uncompressed) -> 13265926, (csv,bzip2) -> 10676889, (json,gzip) -> 14662095, (parquet,uncompressed) -> 8344008, (orc,zlib) -> 4377612)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "data: scala.collection.mutable.Map[(String, String),Long] = Map((orc,snappy) -> 5810818, (json,deflate) -> 14662059, (json,bzip2) -> 9491007, (csv,uncompressed) -> 68804952, (parquet,snappy) -> 6476035, (csv,deflate) -> 11998839, (csv,gzip) -> 11998875, (json,uncompressed) -> 238735998, (parquet,gzip) -> 4820150, (orc,uncompressed) -> 13265926, (csv,bzip2) -> 10676889, (json,gzip) -> 14662095, (parquet,uncompressed) -> 8344008, (orc,zlib) -> 4377612)\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var data = scala.collection.mutable.Map[Tuple2[String, String], Long]()\n",
    "for (line <- sizes) {\n",
    "    val cols = line.split(\",\").map(_.trim)\n",
    "    data += ((cols(0), cols(1)) -> cols(2).toLong)\n",
    "}\n",
    "println(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taille (en Mo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Format | uncomp.  | bzip2    | def.     | gzip     | snappy   | zlib     |\n",
      "csv    |    65.62 |    10.18 |    11.44 |    11.44 |     0.00 |     0.00 |\n",
      "json   |   227.68 |     9.05 |    13.98 |    13.98 |     0.00 |     0.00 |\n",
      "parquet|     7.96 |     0.00 |     0.00 |     4.60 |     6.18 |     0.00 |\n",
      "orc    |    12.65 |     0.00 |     0.00 |     0.00 |     5.54 |     4.17 |\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "fileFormats: List[String] = List(csv, json, parquet, orc)\n",
       "codecs: List[String] = List(uncompressed, bzip2, deflate, gzip, snappy, zlib)\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val fileFormats = List(\"csv\", \"json\", \"parquet\", \"orc\")\n",
    "val codecs = List(\"uncompressed\", \"bzip2\", \"deflate\", \"gzip\", \"snappy\", \"zlib\")\n",
    "println(\"Format | uncomp.  | bzip2    | def.     | gzip     | snappy   | zlib     |\")\n",
    "for (format <- fileFormats) {\n",
    "    print(f\"$format%-7s|\")\n",
    "    for (codec <- codecs) {\n",
    "        val size = data.getOrElse[Long]((format, codec), 0) / (1024.0 * 1024)\n",
    "        print(f\"$size%9.2f |\")\n",
    "    }\n",
    "    println\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
